{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e5d6c33",
   "metadata": {},
   "source": [
    "## Model Development\n",
    "The aim of this notebook is to use our articles dataset (â‰ˆ6000 articles) to train a model than can predict the category of a given article. We will go through 5 steps:\n",
    "1. Data pre-process: Where we are going to use our DataProcessingPipeline in case the dataset is not stored. This pipeline will clean and embed articles allowing us to capture semantic meaning of each article.\n",
    "2. Divide the dataset into 3: Train (70%), Cross-Validation (10%) and Test (20%).\n",
    "3. Hyperparameters search: Once we have our data ready we will searh for hyperparameters in order to find the 'best' model.\n",
    "4. Training: With the hyperparameters we found we will train our `XGBoost` model using 'multi:softprob' in order to get the probability of each category (Maybe an article belong to more than one category).\n",
    "5. Evaluation: We will run our model with the test dataset and evaluate the results in terms of macro and micro, using certain metrics (accuracy, precision, recall and F1-score).\n",
    "\n",
    "After all this steps if the model performs as needed we will save it as `.pkl`, so we can later use it for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce37d69",
   "metadata": {},
   "source": [
    "#### Import all packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0eb280b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import ast\n",
    "sys.path.append('../')  # Adjust path if src is not in notebook's parent directory\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, log_loss\n",
    "\n",
    "from src.data_processing.processing_pipeline import DataProcessingPipeline\n",
    "from src.data_processing.steps.cleaner import Cleaner\n",
    "from src.data_processing.steps.embedder import Embedder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45246fb",
   "metadata": {},
   "source": [
    "#### Step 1 - Data Pre-Processing\n",
    "The idea is that this step won't run every single time but only when we have new data or when we don't have our embedded_dataset available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04838cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fix dataset (it had a bug with float and np.float64)\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def fix_embedding_string(s):\n",
    "    # Replace np.float64(...) with the float value inside\n",
    "    s = re.sub(r'np\\.float64\\(([^)]+)\\)', r'\\1', s)\n",
    "    # Evaluate as a Python list\n",
    "    try:\n",
    "        emb = eval(s)\n",
    "        return [float(x) for x in emb]\n",
    "    except Exception:\n",
    "        # Fallback: extract all numbers\n",
    "        numbers = re.findall(r'[-+]?\\d*\\.\\d+|\\d+', s)\n",
    "        return [float(num) for num in numbers]\n",
    "\n",
    "df = pd.read_csv(\"../data/embedded_dataset.csv\")\n",
    "df['embedding'] = df['embedding'].apply(fix_embedding_string)\n",
    "df['embedding'] = df['embedding'].apply(lambda x: str(x))\n",
    "df.to_csv(\"embedded_dataset_fixed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "217b7e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_list_string(list_string):\n",
    "    try:\n",
    "        return np.array(ast.literal_eval(list_string))\n",
    "    except (ValueError, SyntaxError):\n",
    "        print(\"Malformed embedding string:\", list_string)\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d17753",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../data/embedded_dataset.csv\"\n",
    "is_re_run_active = False\n",
    "\n",
    "if os.path.exists(file_path) and not is_re_run_active:\n",
    "    print(f\"Loading '{file_path}' from repo...\")\n",
    "    df = pd.read_csv(\"../data/embedded_dataset.csv\", converters={'embedding': parse_list_string})\n",
    "else:\n",
    "    print(\"Embeddings dataset was not found.\")\n",
    "    print(\"Loading dataset and running pre-processing...\")\n",
    "\n",
    "    # Load our dataset for pre-processing\n",
    "    df = pd.read_csv(\"../data/dataset.csv\")\n",
    "\n",
    "    # Variables for pre-processing pipeline\n",
    "    MODEL = \"jina/jina-embeddings-v2-base-es:latest\"\n",
    "    CHUNK_SIZE = 3000\n",
    "    CHUNK_OVERLAP = 450\n",
    "    THRESHOLD = 15000\n",
    "\n",
    "    # Creating instance of pipeline\n",
    "    pipeline = DataProcessingPipeline(steps=[\n",
    "        Cleaner(step_name=\"Cleaner\"),\n",
    "        Embedder(model_name=MODEL,\n",
    "                 chunk_size=CHUNK_SIZE,\n",
    "                 chunk_overlap=CHUNK_OVERLAP,\n",
    "                 threshold=THRESHOLD,\n",
    "                 step_name=\"Embedder\")\n",
    "    ])\n",
    "\n",
    "    # Run pipeline\n",
    "    df = pipeline.run(df=df)\n",
    "\n",
    "    # Save results as embeddings dataset\n",
    "    df['embedding'] = df['embedding'].apply(lambda emb: str([float(x) for x in emb]))\n",
    "    df.to_csv(\"../data/embedded_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2534e281",
   "metadata": {},
   "source": [
    "#### Step 2 - Create Train/CV/Test splits\n",
    "We are going to split our dataset as mentioned in train, cross-validation and test sets. In addition we will transform and prepare our categories to be numerical (0...N-1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "794572e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_temp, X_test, y_train_temp, y_test = train_test_split(\n",
    "    df[\"embedding\"], df[\"category\"],\n",
    "    test_size=0.2,\n",
    "    shuffle=True,\n",
    "    stratify=df[\"category\"],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train, X_cv, y_train, y_cv = train_test_split(\n",
    "    X_train_temp, y_train_temp,\n",
    "    test_size=0.15,\n",
    "    shuffle=True,\n",
    "    stratify=y_train_temp,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1aea094",
   "metadata": {},
   "source": [
    "##### Encode labels to be numerical (0...N-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23408fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "# Encode all sets categories using LabelEncoder\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "y_cv = label_encoder.transform(y_cv)\n",
    "y_test = label_encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d45b8d",
   "metadata": {},
   "source": [
    "##### Save label encoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bc23ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(label_encoder, \"../models/label_encoder.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3be161c",
   "metadata": {},
   "source": [
    "##### Convert embedding column to 2D numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d779ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.stack(X_train)\n",
    "X_cv = np.stack(X_cv)\n",
    "X_test = np.stack(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fdabc1",
   "metadata": {},
   "source": [
    "#### Step 3 - Hyperparameters tunning\n",
    "We are first going to train a basic version of the XGBoost model to see the performance without tunning. After that we will search for hyperparameters using GridSearch & RandomizedSearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e712a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train base model without hyperparameters\n",
    "xgb_classifier = xgb.XGBClassifier(objective=\"multi:softprob\", random_state=42)\n",
    "xgb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict using Cross-Validation set\n",
    "y_pred_proba_cv = xgb_classifier.predict_proba(X_cv)\n",
    "y_pred_cv = np.argmax(y_pred_proba_cv, axis=1)\n",
    "\n",
    "print(f\"Cross-Validation accuracy: {accuracy_score(y_cv, y_pred_cv):.4f}\")\n",
    "print(f\"Macro F1 (CV): {f1_score(y_cv, y_pred_cv, average='macro'):.4f}\")\n",
    "print(f\"Log Loss (CV): {log_loss(y_cv, y_pred_proba_cv):.4f}\")\n",
    "print(f\"\\nClassification report (CV):\\n{classification_report(y_cv, y_pred_cv)}\")\n",
    "print(f\"Confusion matrix (CV):\\n{confusion_matrix(y_cv, y_pred_cv)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c64cd0",
   "metadata": {},
   "source": [
    "Let's start the hyperparameter search, we will create a grid containing all params needed:\n",
    "* `max_depth`: Maximum depth of a tree.\n",
    "* `learning_rate`: Rate that the model changes the weigths.\n",
    "* `n_estimators`: Number of trees in the model.\n",
    "* `subsample`: Fraction of training instances to be used for each tree.\n",
    "* `reg_alpha`: L1 regularization.\n",
    "* `reg_lambda`: L2 regularization.\n",
    "* `min_child_weight`: Used to control over-fitting.\n",
    "* `colsample_bytree`: Reduces overfitting by introducing randomness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e0d2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"max_depth\": [3, 4, 5, 6],\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1, 0.3],\n",
    "    \"n_estimators\": [200, 400, 600, 800],\n",
    "    \"subsample\": [0.6, 0.8, 1.0],\n",
    "    \"reg_alpha\": [0, 0.001, 0.01, 0.1, 1, 10],\n",
    "    \"reg_lambda\": [0, 0.1, 0.5, 1, 2],\n",
    "    \"min_child_weight\": [1, 3, 5, 10],\n",
    "    \"colsample_bytree\": [0.6, 0.8, 1.0],\n",
    "}\n",
    "\n",
    "# Create a new XGBoost instance\n",
    "xgb_model = xgb.XGBClassifier(objective=\"multi:softprob\", random_state=42)\n",
    "# Set up randomized search\n",
    "randomized_search = RandomizedSearchCV(xgb_model, param_grid, cv=3, scoring=\"accuracy\", n_iter=30, n_jobs=-1, verbose=2, random_state=42)\n",
    "randomized_search.fit(X_train, y_train)\n",
    "# Best xgb model obtained\n",
    "best_xgb = randomized_search.best_estimator_\n",
    "\n",
    "print(f\"Best Parameters: {randomized_search.best_params_}\")\n",
    "print(f\"Best Accuracy: {randomized_search.best_score_}\")\n",
    "# {'subsample': 0.6, 'reg_lambda': 0.1, 'reg_alpha': 0, 'n_estimators': 600, 'min_child_weight': 1, 'max_depth': 6, 'learning_rate': 0.1, 'colsample_bytree': 0.6}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb403561",
   "metadata": {},
   "source": [
    "##### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14a362f",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(best_xgb, '../models/xgboost_model_v1.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3b1fe2",
   "metadata": {},
   "source": [
    "#### Step 4 - Evaluation\n",
    "Now that we have our 'best' model we are going to predict our test data and get the metrics in order to evaluate the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c507602d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict test set using 'best' model (XGBoost)\n",
    "y_pred_proba = best_xgb.predict_proba(X_test)\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "\n",
    "print(f\"Test accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Macro F1 (Test): {f1_score(y_test, y_pred, average='macro'):.4f}\")\n",
    "print(f\"Log Loss (Test): {log_loss(y_test, y_pred_proba):.4f}\")\n",
    "print(f\"\\nClassification report (Test):\\n{classification_report(y_test, y_pred)}\")\n",
    "print(f\"Confusion matrix (Test):\\n{confusion_matrix(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8306fe99",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "After pre-processing the dataset, train a XGBoost model and predict using the 'best' model (After the hyperparameter search), we got the following results.\n",
    "\n",
    "Cross-Valiadtion evaluation:\n",
    "* Cross-Validation accuracy: 0.8950\n",
    "* Macro F1 (CV): 0.8944\n",
    "* Log Loss (CV): 0.3298\n",
    "\n",
    "Test set evaluation:\n",
    "* Test accuracy: 0.8992\n",
    "* Macro F1 (Test): 0.8986\n",
    "* Log Loss (Test): 0.3620\n",
    "\n",
    "And generally across classes it's balanced in terms of performance, class #0 and class #6 tend to be a little bit below the rest but not by a lot. Of course we will keep trying to improve this model, but we consider that almost 90% it's a great result."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
